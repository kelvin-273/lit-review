@book{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come a vast amount of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics.},
address = {New York, NY},
archivePrefix = {arXiv},
arxivId = {1010.3003},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {The Mathematical Intelligencer},
doi = {10.1007/978-0-387-84858-7},
eprint = {1010.3003},
file = {:home/kelvin/Documents/PDFs/ESLII.pdf:pdf},
isbn = {978-0-387-84857-0},
issn = {03436993},
keywords = {inger series in statistics},
number = {2},
pages = {XXII, 745},
pmid = {21196786},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf{\%}255Cnhttp://www-stat.stanford.edu/{~}tibs/book/preface.ps http://link.springer.com/10.1007/978-0-387-84858-7},
volume = {27},
year = {2009}
}
@book{Yitzhaki2013,
address = {New York, NY},
author = {Yitzhaki, Shlomo and Schechtman, Edna},
doi = {10.1007/978-1-4614-4720-7},
edition = {1},
file = {:home/kelvin/Documents/PDFs/978-1-4614-4720-7.pdf:pdf},
isbn = {978-1-4614-4719-1},
keywords = {ANOGI,ANOVA,Economic models,GMD,Gini coefficient,Gini's mean difference,Ordinary Least Squares,Regression},
pages = {1--583},
publisher = {Springer New York},
series = {Springer Series in Statistics},
title = {{The Gini Methodology}},
url = {http://link.springer.com/10.1007/978-1-4614-4720-7},
volume = {272},
year = {2013}
}
@article{Shannon1948,
abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist and Hartley on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure.},
archivePrefix = {arXiv},
arxivId = {chao-dyn/9411012},
author = {Shannon, Claude E},
doi = {10.1145/584091.584093},
eprint = {9411012},
file = {:home/kelvin/Documents/PDFs/entropy.pdf:pdf},
isbn = {0252725484},
issn = {07246811},
journal = {The Bell System Technical Journal},
number = {July 1928},
pages = {379--423},
pmid = {9230594},
primaryClass = {chao-dyn},
title = {{A mathematical theory of communication}},
url = {http://cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf},
volume = {27},
year = {1948}
}
@misc{Clements1992,
author = {Clements, Ron and Musker, John},
publisher = {Buena Vista Pictures},
title = {{Aladdin}},
year = {1992}
}
@book{Du2016,
abstract = {This textbook provides a comprehensive introduction to nature-inspired metaheuristic methods for search and optimization, including the latest trends in evolutionary algorithms and other forms of natural computing. Over 100 different types of these methods are discussed in detail. The authors emphasize non-standard optimization problems and utilize a natural approach to the topic, moving from basic notions to more complex ones. An introductory chapter covers the necessary biological and mathematical backgrounds for understanding the main material. Subsequent chapters then explore almost all of the major metaheuristics for search and optimization created based on natural phenomena, including simulated annealing, recurrent neural networks, genetic algorithms and genetic programming, differential evolution, memetic algorithms, particle swarm optimization, artificial immune systems, ant colony optimization, tabu search and scatter search, bee and bacteria foraging algorithms, harmony search, biomolecular computing, quantum computing, and many others. General topics on dynamic, multimodal, constrained, and multiobjective optimizations are also described. Each chapter includes detailed flowcharts that illustrate specific algorithms and exercises that reinforce important topics. Introduced in the appendix are some benchmarks for the evaluation of metaheuristics. Search and Optimization by Metaheuristics is intended primarily as a textbook for graduate and advanced undergraduate students specializing in engineering and computer science. It will also serve as a valuable resource for scientists and researchers working in these areas, as well as those who are interested in search and optimization methods. {\textcopyright} Springer International Publishing Switzerland 2016. All rights reserved.},
author = {Du, Ke-Lin and Swamy, M. N. S.},
doi = {10.1007/978-3-319-41192-7},
file = {:home/kelvin/Documents/PDFs/Search-and-Optimisation-by-Meta.pdf:pdf},
isbn = {978-3-319-41191-0},
issn = {3319411926},
title = {{Search and Optimization by Metaheuristics}},
url = {http://link.springer.com/10.1007/978-3-319-41192-7},
year = {2016}
}
@article{Li1975,
author = {Li, C L and McCormick, S T and Simchi-Levi, D},
doi = {https://doi.org/10.1002/net.1975.5.1.45},
journal = {Networks},
title = {{On the Computational Complexity of Combinatorial Problems}},
year = {1975}
}
@article{Ly2017,
abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; finally, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
archivePrefix = {arXiv},
arxivId = {1705.01064},
author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul P.P.P. and Wagenmakers, Eric Jan},
doi = {10.1016/j.jmp.2017.05.006},
eprint = {1705.01064},
file = {:home/kelvin/Desktop/books/fisher-info.pdf:pdf},
issn = {10960880},
journal = {Journal of Mathematical Psychology},
keywords = {Confidence intervals,Hypothesis testing,Jeffreys's prior,Minimum description length,Model complexity,Model selection,Statistical modeling},
pages = {40--55},
title = {{A Tutorial on Fisher information}},
volume = {80},
year = {2017}
}
@book{Conforti2014,
abstract = {This book is an elegant and rigorous presentation of integer programming, exposing the subject's mathematical depth and broad applicability. Special attention is given to the theory behind the algorithms used in state-of-the-art solvers. An abundance of concrete examples and exercises of both theoretical and real-world interest explore the wide range of applications and ramifications of the theory. Each chapter is accompanied by an expertly informed guide to the literature and special topics, rounding out the reader's understanding and serving as a gateway to deeper study. Key topics include: formulations polyhedral theory cutting planes decomposition enumeration semidefinite relaxations Written by renowned experts in integer programming and combinatorial optimization, Integer Programming is destined to become an essential text in the field.},
author = {Conforti, Michele and Cornu{\'{e}}jols, G{\'{e}}rard and Zambelli, Giacomo},
booktitle = {Encyclopedia of Optimization},
doi = {10.1007/0-306-48332-7_212},
file = {:home/kelvin/Documents/UNI/Honurs/TH/refs/978-3-319-11008-0.pdf:pdf},
isbn = {978-0-387-23460-1},
issn = {1471-2105},
keywords = {Integer Programming,Mixed-Integer Linear Programming,Operations Research,Optimization,Polyhedral Theory},
pages = {1--466},
pmid = {21172054},
publisher = {Springer, Cham},
title = {{Integer Programming}},
url = {https://link.springer.com/book/10.1007/978-3-319-11008-0},
volume = {271},
year = {2014}
}
@book{Wallace2005,
abstract = {The Minimum Message Length (MML) Principle is an information-theoretic approach to induction, hypothesis testing, model selection, and statistical inference. MML, which provides a formal specification for the implementation of Occam's Razor, asserts that the â€˜best' explanation of observed data is the shortest. Further, an explanation is acceptable (i.e. the induction is justified) only if the explanation is shorter than the original data. This book gives a sound introduction to the Minimum Message Length Principle and its applications, provides the theoretical arguments for the adoption of the principle, and shows the development of certain approximations that assist its practical application. MML appears also to provide both a normative and a descriptive basis for inductive reasoning generally, and scientific induction in particular. The book describes this basis and aims to show its relevance to the Philosophy of Science. Statistical and Inductive Inference by Minimum Message Length will be of special interest to graduate students and researchers in Machine Learning and Data Mining, scientists and analysts in various disciplines wishing to make use of computer techniques for hypothesis discovery, statisticians and econometricians interested in the underlying theory of their discipline, and persons interested in the Philosophy of Science. The book could also be used in a graduate-level course in Machine Learning and Estimation and Model-selection, Econometrics and Data Mining. C.S. Wallace was appointed Foundation Chair of Computer Science at Monash University in 1968, at the age of 35, where he worked until his death in 2004. He received an ACM Fellowship in 1995, and was appointed Professor Emeritus in 1996. Professor Wallace made numerous significant contributions to diverse areas of Computer Science, such as Computer Architecture, Simulation and Machine Learning. His final research focused primarily on the Minimum Message Length Principle.},
author = {Wallace, C S},
doi = {10.1007/0-387-27656-4},
file = {:home/kelvin/Desktop/books/Statistical and Inductive Inference by Minimum Message Length.pdf:pdf},
isbn = {038723795X},
pages = {429},
pmid = {13754104},
title = {{Statistical and Inductive Inference by Minimum Message Length}},
url = {http://books.google.com/books?id=3NmFwNHaNbUC{\&}pgis=1},
year = {2005}
}
@article{Belov2017,
abstract = {We show how recently-defined abstract models of the Branch {\&} Bound algorithm can be used to ob-tain information on how the nodes are distributed in B{\&}B search trees. This can be directly exploited in the form of probabilities in a sampling algorithm given by Knuth that estimates the size of a search tree. This method reduces the offline estimation er-ror by a factor of two on search trees from Mixed-Integer Programming instances.},
author = {Belov, Gleb and Esler, Samuel and Fernando, Dylan and Bodic, Pierre Le and Nemhauser, George L.},
doi = {10.24963/ijcai.2017/67},
file = {:home/kelvin/Documents/PDFs/0067.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Combinatorial {\&} Heuristic Search: Combinatorial se,Constraints and Satisfiability: Solvers and Tools},
pages = {473--479},
title = {{Estimating the size of search trees by sampling with domain knowledge}},
year = {2017}
}
@article{Khalil2017a,
abstract = {" Primal heuristics " are a key contributor to the improved performance of exact branch-and-bound solvers for combinatorial optimization and integer programming. Perhaps the most crucial question concerning primal heuristics is that of at which nodes they should run, to which the typical answer is via hard-coded rules or fixed solver parameters tuned, offline, by trial-and-error. Alternatively, a heuristic should be run when it is most likely to succeed, based on the problem instance's charac-teristics, the state of the search, etc. In this work, we study the problem of deciding at which node a heuristic should be run, such that the overall (pri-mal) performance of the solver is optimized. To our knowledge, this is the first attempt at formalizing and systematically addressing this problem. Cen-tral to our approach is the use of Machine Learning (ML) for predicting whether a heuristic will suc-ceed at a given node. We give a theoretical frame-work for analyzing this decision-making process in a simplified setting, propose a ML approach for modeling heuristic success likelihood, and design practical rules that leverage the ML models to dy-namically decide whether to run a heuristic at each node of the search tree. Experimentally, our ap-proach improves the primal performance of a state-of-the-art Mixed Integer Programming solver by up to 6{\%} on a set of benchmark instances, and by up to 60{\%} on a family of hard Independent Set instances.},
author = {Khalil, Elias B. and Dilkina, Bistra and Nemhauser, George L. and Ahmed, Shabbir and Shao, Yufen},
doi = {10.24963/ijcai.2017/92},
file = {:home/kelvin/Documents/PDFs/0092.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Combinatorial {\&} Heuristic Search: Combinatorial se,Constraints and Satisfiability: Constraint Optimis,Constraints and Satisfiability: Constraint Satisfa,Constraints and Satisfiability: Constraints and Sa},
pages = {659--666},
title = {{Learning to run heuristics in tree search}},
year = {2017}
}
@article{Vigerske2017,
abstract = {This paper describes the extensions that were added to the constraint integer programming framework SCIP in order to enable it to solve convex and nonconvex mixed-integer nonlinear programs (MINLPs) to global optimality. SCIP implements a spatial branch-and-bound algorithm based on a linear outer-approximation, which is computed by convex over- and underestimation of nonconvex functions. An expression graph representation of nonlinear constraints allows for bound tightening, structure analysis, and reformulation. Primal heuristics are employed throughout the solving process to find feasible solutions early. We provide insights into the performance impact of individual MINLP solver components via a detailed computational study over a large and heterogeneous test set.},
author = {Vigerske, Stefan and Gleixner, Ambros},
doi = {10.1080/10556788.2017.1335312},
file = {:home/kelvin/Documents/PDFs/SCIP global optimization of mixed integer nonlinear programs in a branch and cut framework.pdf:pdf},
issn = {10294937},
journal = {Optimization Methods and Software},
keywords = {MINLP,MIQCP,global optimization,mixed-integer nonlinear programming,mixed-integer quadratically constrained programmin,nonconvex constraints},
pages = {1--31},
title = {{SCIP: global optimization of mixed-integer nonlinear programs in a branch-and-cut framework}},
url = {https://doi.org/10.1080/10556788.2017.1335312},
volume = {6788},
year = {2017}
}
@article{Khalil2017,
abstract = {" Primal heuristics " are a key contributor to the improved performance of exact branch-and-bound solvers for combinatorial optimization and integer programming. Perhaps the most crucial question concerning primal heuristics is that of at which nodes they should run, to which the typical answer is via hard-coded rules or fixed solver parameters tuned, offline, by trial-and-error. Alternatively, a heuristic should be run when it is most likely to succeed, based on the problem instance's charac-teristics, the state of the search, etc. In this work, we study the problem of deciding at which node a heuristic should be run, such that the overall (pri-mal) performance of the solver is optimized. To our knowledge, this is the first attempt at formalizing and systematically addressing this problem. Cen-tral to our approach is the use of Machine Learning (ML) for predicting whether a heuristic will suc-ceed at a given node. We give a theoretical frame-work for analyzing this decision-making process in a simplified setting, propose a ML approach for modeling heuristic success likelihood, and design practical rules that leverage the ML models to dy-namically decide whether to run a heuristic at each node of the search tree. Experimentally, our ap-proach improves the primal performance of a state-of-the-art Mixed Integer Programming solver by up to 6{\%} on a set of benchmark instances, and by up to 60{\%} on a family of hard Independent Set instances.},
author = {Khalil, Elias B. and Dilkina, Bistra and Nemhauser, George L. and Ahmed, Shabbir and Shao, Yufen},
doi = {10.24963/ijcai.2017/92},
file = {:home/kelvin/Documents/PDFs/0092.pdf:pdf},
isbn = {9780999241103},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Combinatorial {\&} Heuristic Search: Combinatorial se,Constraints and Satisfiability: Constraint Optimis,Constraints and Satisfiability: Constraint Satisfa,Constraints and Satisfiability: Constraints and Sa},
pages = {659--666},
title = {{Learning to run heuristics in tree search}},
year = {2017}
}
@article{Balcan2018,
abstract = {Tree search algorithms, such as branch-and-bound, are the most widely used tools for solving combinatorial and nonconvex problems. For example, they are the foremost method for solving (mixed) integer programs and constraint satisfaction problems. Tree search algorithms recursively partition the search space to find an optimal solution. In order to keep the tree size small, it is crucial to carefully decide, when expanding a tree node, which question (typically variable) to branch on at that node in order to partition the remaining space. Numerous partitioning techniques (e.g., variable selection) have been proposed, but there is no theory describing which technique is optimal. We show how to use machine learning to determine an optimal weighting of any set of partitioning procedures for the instance distribution at hand using samples from the distribution. We provide the first sample complexity guarantees for tree search algorithm configuration. These guarantees bound the number of samples sufficient to ensure that the empirical performance of an algorithm over the samples nearly matches its expected performance on the unknown instance distribution. This thorough theoretical investigation naturally gives rise to our learning algorithm. Via experiments, we show that learning an optimal weighting of partitioning procedures can dramatically reduce tree size, and we prove that this reduction can even be exponential. Through theory and experiments, we show that learning to branch is both practical and hugely beneficial.},
archivePrefix = {arXiv},
arxivId = {1803.10150},
author = {Balcan, Maria-Florina and Dick, Travis and Sandholm, Tuomas and Vitercik, Ellen},
eprint = {1803.10150},
file = {:home/kelvin/Documents/PDFs/1803.10150.pdf:pdf},
pages = {1--35},
title = {{Learning to Branch}},
url = {http://arxiv.org/abs/1803.10150},
year = {2018}
}
@article{Lodi2017,
abstract = {This paper surveys learning techniques to deal with the two most crucial decisions in the branch-and-bound algorithm for Mixed-Integer Linear Programming, namely variable and node selections. Because of the lack of deep mathematical under-standing on those decisions, the classical and vast literature in the field is inherently based on computational studies and heuristic, often problem-specific, strategies. We will both interpret some of those early contributions in the light of modern (machine) learning techniques, and give the details of the recent algorithms that instead explicitly incorporate machine learning paradigms.},
author = {Lodi, Andrea and Zarpellon, Giulia},
doi = {10.1007/s11750-017-0451-6},
file = {:home/kelvin/Documents/PDFs/lodi2017.pdf:pdf},
isbn = {1175001704543},
issn = {18638279},
journal = {TOP},
keywords = {Branch and bound,Machine learning},
number = {2},
pages = {207--236},
title = {{On learning and branching: a survey}},
volume = {25},
year = {2017}
}
@inproceedings{Khalil2017b,
abstract = {" Primal heuristics " are a key contributor to the improved performance of exact branch-and-bound solvers for combinatorial optimization and integer programming. Perhaps the most crucial question concerning primal heuristics is that of at which nodes they should run, to which the typical answer is via hard-coded rules or fixed solver parameters tuned, offline, by trial-and-error. Alternatively, a heuristic should be run when it is most likely to succeed, based on the problem instance's charac-teristics, the state of the search, etc. In this work, we study the problem of deciding at which node a heuristic should be run, such that the overall (pri-mal) performance of the solver is optimized. To our knowledge, this is the first attempt at formalizing and systematically addressing this problem. Cen-tral to our approach is the use of Machine Learning (ML) for predicting whether a heuristic will suc-ceed at a given node. We give a theoretical frame-work for analyzing this decision-making process in a simplified setting, propose a ML approach for modeling heuristic success likelihood, and design practical rules that leverage the ML models to dy-namically decide whether to run a heuristic at each node of the search tree. Experimentally, our ap-proach improves the primal performance of a state-of-the-art Mixed Integer Programming solver by up to 6{\%} on a set of benchmark instances, and by up to 60{\%} on a family of hard Independent Set instances.},
author = {Khalil, Elias B. and Dilkina, Bistra and Nemhauser, George L. and Ahmed, Shabbir and Shao, Yufen},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2017/92},
isbn = {9780999241103},
issn = {10450823},
title = {{Learning to run heuristics in tree search}},
year = {2017}
}
@article{LeBodic2018,
abstract = {Augmentation methods for mixed-integer (linear) programs are a class of primal solution approaches in which a feasible solution is iteratively augmented to a better solution or proved to be optimal. It is well known that the performance of these methods, i.e., number of iterations needed, can theoretically be improved by scaling methods. We extend these results by an improved and extended convergence analysis, which shows that bit scaling and geometric scaling theoretically perform identically well in the worst case for 0/1 polytopes, as well as show that in some cases, geometric scaling can outperform bit scaling arbitrarily, leading to the first strong separation between these two methods. We also investigate the performance of implementations of these methods, where the augmentation directions are computed by a MIP solver. It turns out that the number of required iterations is low in most cases. While scaling methods usually do not improve the performance for easier problems, in the case of hard mixed-integer optimization problems they allow to compute solutions of very good quality and are often superior.},
archivePrefix = {arXiv},
arxivId = {1509.03206},
author = {{Le Bodic}, Pierre and Pavelka, Jeffrey W. and Pfetsch, Marc E. and Pokutta, Sebastian},
doi = {10.1016/j.disopt.2017.08.004},
eprint = {1509.03206},
file = {:home/kelvin/Documents/PDFs/1-s2.0-S1572528617301895-main.pdf:pdf},
issn = {15725286},
journal = {Discrete Optimization},
keywords = {Augmentation methods,Mixed-integer programs,Primal methods,Scaling},
pages = {1--25},
publisher = {Elsevier B.V.},
title = {{Solving MIPs via scaling-based augmentation}},
url = {http://dx.doi.org/10.1016/j.disopt.2017.08.004},
volume = {27},
year = {2018}
}
@article{LeBodic2017,
abstract = {The selection of branching variables is a key component of branch-and-bound algorithms for solving Mixed-Integer Programming (MIP) problems since the quality of the selection procedure is likely to have a significant effect on the size of the enumeration tree. State-of-the-art procedures base the selection of variables on their "LP gains", which is the dual bound improvement obtained after branching on a variable. There are various ways of selecting variables depending on their LP gains. However, all methods are evaluated empirically. In this paper we present a theoretical model for the selection of branching variables. It is based upon an abstraction of MIPs to a simpler setting in which it is possible to analytically evaluate the dual bound improvement of choosing a given variable. We then discuss how the analytical results can be used to choose branching variables for MIPs, and we give experimental results that demonstrate the effectiveness of the method on MIPLIB 2010 "tree" instances where we achieve a 5{\%} geometric average time and node improvement over the default rule of SCIP, a state-of-the-art MIP solver.},
archivePrefix = {arXiv},
arxivId = {1511.01818},
author = {{Le Bodic}, Pierre and Nemhauser, George},
doi = {10.1007/s10107-016-1101-8},
eprint = {1511.01818},
file = {:home/kelvin/Documents/PDFs/s10107-016-1101-8.pdf:pdf},
isbn = {1010701711},
issn = {14364646},
journal = {Mathematical Programming},
keywords = {Abstract model,Algorithm analysis,Branch and bound,Computational complexity,Mixed integer programming},
number = {1-2},
pages = {369--405},
publisher = {Springer Berlin Heidelberg},
title = {{An abstract model for branching and its application to mixed integer programming}},
volume = {166},
year = {2017}
}
@article{Khalil2016,
abstract = {The design of strategies for branching in Mixed Integer Programming (MIP) is guided by cycles of parameter tun-ing and offline experimentation on an extremely heteroge-neous testbed, using the average performance. Once devised, these strategies (and their parameter settings) are essentially input-agnostic. To address these issues, we propose a ma-chine learning (ML) framework for variable branching in MIP. Our method observes the decisions made by Strong Branching (SB), a time-consuming strategy that produces small search trees, collecting features that characterize the candidate branching variables at each node of the tree. Based on the collected data, we learn an easy-to-evaluate surrogate function that mimics the SB strategy, by means of solving a learning-to-rank problem, common in ML. The learned ranking function is then used for branching. The learning is instance-specific, and is performed on-the-fly while executing a branch-and-bound search to solve the instance. Experiments on benchmark instances indicate that our method produces significantly smaller search trees than existing heuristics, and is competitive with a state-of-the-art commercial solver.},
author = {Khalil, Elias B and Bodic, Pierre Le and Song, Le and Nemhauser, George and Dilkina, Bistra},
file = {:home/kelvin/Documents/PDFs/12514-55524-1-PB.pdf:pdf},
isbn = {9781577357605},
journal = {Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016)},
keywords = {Technical Papers: Heuristic Search and Optimizatio},
title = {{Learning to Branch in Mixed Integer Programming}},
year = {2016}
}
@article{LeBodic2015,
abstract = {We show the importance of selecting good branching variables by exhibiting a family of instances for which an optimal solution is both trivial to find and provably optimal by a fixed-size branch-and-bound tree, but for which state-of-the-art Mixed Integer Programming solvers need an increasing amount of resources. The instances encode the edge-coloring problem on a family of graphs containing a small subgraph requiring four colors, while the rest of the graph requires only three.},
author = {{Le Bodic}, Pierre and Nemhauser, George L.},
doi = {10.1016/j.orl.2015.03.003},
file = {:home/kelvin/Documents/PDFs/1-s2.0-S0167637715000413-main.pdf:pdf},
issn = {01676377},
journal = {Operations Research Letters},
keywords = {Branch and bound,Chromatic index,Edge coloring,Mixed integer programming solvers,Tree size},
number = {3},
pages = {273--278},
publisher = {Elsevier B.V.},
title = {{How important are branching decisions: Fooling MIP solvers}},
url = {http://dx.doi.org/10.1016/j.orl.2015.03.003},
volume = {43},
year = {2015}
}
@article{Achterberg2005,
abstract = {We present a new generalization called reliability branching of today's state-of-the-art strong branching and pseudocost branching strategies for linear programming based branch-and-bound algorithms. After reviewing commonly used branching strategies and performing extensive computational studies we compare different parameter settings and show the superiority of our proposed new strategy. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Achterberg, Tobias and Koch, Thorsten and Martin, Alexander},
doi = {10.1016/j.orl.2004.04.002},
file = {:home/kelvin/Documents/PDFs/1-s2.0-S0167637704000501-main.pdf:pdf},
issn = {01676377},
journal = {Operations Research Letters},
keywords = {Branch-and-bound,Mixed-integer-programming,Pseudocost-branching,Reliability-branching,Strong-branching,Variable selection},
number = {1},
pages = {42--54},
title = {{Branching rules revisited}},
volume = {33},
year = {2005}
}
@article{Ibaraki1976,
abstract = {Four known search strategies used in branch-and-bound algorithms-heuristic search, depth-first search, best-bound search, and breadth-first search-are theoretically compared from the viewpoint of the performance of the resulting algorithms. Heuristic search includes the other three as special cases. Since heuristic search is determined by a heuristic function h, we first investigate how the performance of the resulting algorithms depends on h. In particular, we show that heuristic search is stable in the sense that a slight change in h causes only a slight change in its performance. The "best" and the "worst" heurstic functions are clarified, and also discussed is how the heuristic function h should be modified to obtain a branch-and-bound algorithm with an improved performance. Finally, properties and limitations of depth-first search, best-bound search, and breadth-first search viewed as special cases of heuristic search are considered. In particular, it is shown that the stability observed for heuristic search no longer holds for depth-first search. {\textcopyright} 1976 Plenum Publishing Corporation.},
author = {Ibaraki, Toshihide},
doi = {10.1007/BF00998631},
file = {:home/kelvin/Documents/PDFs/ibaraki1976.pdf:pdf},
issn = {00917036},
journal = {International Journal of Computer {\&} Information Sciences},
keywords = {Branch-and-bound,best-bound search,breadth-first search,computational efficiency,depth-first search,heuristic search,search strategies},
number = {4},
pages = {315--344},
title = {{Theoretical comparisons of search strategies in branch-and-bound algorithms}},
volume = {5},
year = {1976}
}
@article{Gilpin2011,
abstract = {Deciding what question to branch on at each node is a key element of search algorithms. In this paper, we describe a collection of techniques for branching decisions that are motivated from an information-theoretic perspective. The idea is to drive the search to reduce the uncertainty (entropy) in the current subproblem. We present four families of methods for branch question selection in mixed integer programming that use this idea. In the first, a variable to branch on is selected based on lookahead. This method performs comparably to strong branching on MIPLIB, and better than strong branching on hard real-world procurement optimization instances on which CPLEX's default strong branching outperforms CPLEX's default branching strategy. The second family combines this idea with strong branching. The third family does not use lookahead, but instead exploits the tie between indicator variables and the variables they govern. This significantly outperforms the state-of-the-art branching strategies on both combinatorial procurement problems and facility location problems. The fourth family concerns branching using carefully constructed linear inequality constraints over sets of variables. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Gilpin, Andrew and Sandholm, Tuomas},
doi = {10.1016/j.disopt.2010.07.001},
file = {:home/kelvin/Documents/PDFs/1-s2.0-S1572528610000423-main.pdf:pdf},
isbn = {1595933034},
issn = {15725286},
journal = {Discrete Optimization},
keywords = {Branch-and-bound,Branching heuristic,Entropic branching,Mixed-integer programming,Search},
number = {2},
pages = {147--159},
publisher = {Elsevier B.V.},
title = {{Information-theoretic approaches to branching in search}},
url = {http://dx.doi.org/10.1016/j.disopt.2010.07.001},
volume = {8},
year = {2011}
}
@article{Morrison2016,
abstract = {The branch-and-bound (B{\&}B) algorithmic framework has been used successfully to find exact solutions for a wide array of optimization problems. B{\&}B uses a tree search strategy to implicitly enumerate all possible solutions to a given problem, applying pruning rules to eliminate regions of the search space that cannot lead to a better solution. There are three algorithmic components in B{\&}B that can be specified by the user to fine-tune the behavior of the algorithm. These components are the search strategy, the branching strategy, and the pruning rules. This survey presents a description of recent research advances in the design of B{\&}B algorithms, particularly with regards to these three components. Moreover, three future research directions are provided in order to motivate further exploration in these areas.},
author = {Morrison, David R. and Jacobson, Sheldon H. and Sauppe, Jason J. and Sewell, Edward C.},
doi = {10.1016/j.disopt.2016.01.005},
file = {:home/kelvin/Documents/PDFs/1-s2.0-S1572528616000062-main.pdf:pdf},
issn = {15725286},
journal = {Discrete Optimization},
keywords = {Branch-and-bound,Cyclic best first search,Discrete optimization,Integer programming,Search strategies,Survey},
pages = {79--102},
publisher = {Elsevier B.V.},
title = {{Branch-and-bound algorithms: A survey of recent advances in searching, branching, and pruning}},
url = {http://dx.doi.org/10.1016/j.disopt.2016.01.005},
volume = {19},
year = {2016}
}
@article{Kent1983,
abstract = {Given a parametric model of dependence between two random quantities, X and Y, the notion of information gain can be used to define a measure of correlation. This definition of correlation generalizes both the usual product-moment correlation coeffi- cient for the bivariate normal model and the multiple correlation coefficient in the standard linear regression model. The use of this information-based correlation in a descriptive statistical analysis is examined and several examples are given. Some},
author = {Kent, John T.},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trust - 2016 - Biometrika Trust Information Gain and a General Measure of Correlation Author ( s ) John T . Kent Published by Oxford Un.pdf:pdf},
journal = {Biometrika},
keywords = {Akaike's information criterion,Conditional correlation,Joint correlation,Kullback--Leibler information gain,Likelihood ratio test,Robustness},
number = {1},
pages = {163--173},
title = {{Information Gain and a General Measure of Correlation}},
url = {http://www.jstor.org/stable/2335954},
volume = {70},
year = {1983}
}
@article{Mu2004,
author = {Mu, Carlos},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mu - 2004 - A study of the lot â€“ sizing polytope.pdf:pdf},
journal = {Dimension Contemporary German Arts And Letters},
pages = {443--465},
title = {{A study of the lot â€“ sizing polytope}},
volume = {465},
year = {2004}
}
@article{Realff1998,
abstract = {The goal of this article is to present a methodology for the automatic acquisition of new control knowledge in the form of dominance and equivalence conditions, using an explanation-based learning algorithm from artificial intelligence. The acquisition of new control knowledge proceeds through the following stages: 1) Analysis of the problem-solving activity, generated from the application of a branch and bound algorithm on specific instance(s) of a class of problems. 2) Interpretation of the problem-solving activity and synthesis of new control knowledge, which can lead to more efficient solution of future problems. The generated control knowledge is provably correct within the theory of the given class of problems and the employed branch and bound strategy. Consequently, the number of nodes evaluated by a specific branch and bound algorithm is guaranteed to be reduced, as new control knowledge is continuously acquired from the solution of specific problems. The article presents the theoretical foundations for the handling of the above two tasks. It concludes with an application of the proposed approach to a mixed integer linear programming formulation of a batch scheduling problem.},
author = {Realff, Matthew J. and Stephanopoulos, George},
doi = {10.1287/ijoc.10.1.56},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/de Propris, Realff, Stephanopoulos - 1998 - Copyright {\textcopyright} 2000. All rights reserved.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {INFORMS Journal on Computing},
number = {1},
pages = {56--71},
title = {{On the application of explanation-based learning to acquire control knowledge for branch and bound algorithms}},
volume = {10},
year = {1998}
}
@article{Land1960,
abstract = {In the classical linear programming problem the behaviour of continuous, nonnegative variables subject to a system of linear inequalities is investigated. One possible generalization of this problem is to relax the continuity condi- tion on the variables. This paper presents a simple numerical algorithm for the solution of programming problems in which some or all of the variables can take only discrete values. The algorithm requires no special techniques beyond those used in ordinary linear programming, and lends itself to automatic computing. Its use is illustrated on two numerical examples. 1.},
author = {Land, A. H. and Doig, A. G.},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Land, Doig - 1960 - An Automatic Method of Solving Discrete Programming Problems.pdf:pdf},
journal = {Econometrica},
number = {3},
pages = {497--520},
title = {{An Automatic Method of Solving Discrete Programming Problems}},
volume = {28},
year = {1960}
}
@article{He2014,
abstract = {Branch-and-bound is a widely used method in combinatorial optimization, in-cluding mixed integer programming, structured prediction and MAP inference. While most work has been focused on developing problem-specific techniques, little is known about how to systematically design the node searching strategy on a branch-and-bound tree. We address the key challenge of learning an adap-tive node searching order for any class of problem solvable by branch-and-bound. Our strategies are learned by imitation learning. We apply our algorithm to linear programming based branch-and-bound for solving mixed integer programs (MIP). We compare our method with one of the fastest open-source solvers, SCIP; and a very efficient commercial solver, Gurobi. We demonstrate that our approach achieves better solutions faster on four MIP libraries.},
author = {He, He and Daum, Hal and Eisner, Jason},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Daum, Eisner - 2014 - Learning to Search in Branch-and-Bound Algorithms.pdf:pdf},
issn = {10495258},
journal = {Nips},
number = {0964681},
pages = {1--11},
title = {{Learning to Search in Branch-and-Bound Algorithms}},
year = {2014}
}
@article{Clausen1999,
abstract = {A large number of real-world planning problems called combinatorial optimization problems share the following properties: They are optimiza- tion problems, are easy to state, and have a finite but usually very large number of feasible solutions. While some of these as e.g. the Shortest Path problem and the Minimum Spanning Tree problem have polynomial algo- ritms, the majority of the problems in addition share the property that no polynomial method for their solution is known. Examples here are vehicle routing, crew scheduling, and production planning. All of these problems are NP-hard. Branch and Bound (B{\&}B) is by far the most widely used tool for solv- ing large scale NP-hard combinatorial optimization problems. B{\&}B is, however, an algorithm paradigm, which has to be filled out for each spe- cific problem type, and numerous choices for each of the components ex- ist. Even then, principles for the design of efficient B{\&}B algorithms have emerged over the years. In this paper I review the main principles of B{\&}B and illustrate the method and the different design issues through three examples: the Sym- metric Travelling Salesman Problem, the Graph Partitioning problem, and the Quadratic Assignment problem.},
author = {Clausen, Jens},
doi = {10.1.1.5.7475},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Clausen - 1999 - Branch and bound algorithms-principles and examples.pdf:pdf},
journal = {Department of Computer Science, University of {\ldots}},
pages = {1--30},
title = {{Branch and bound algorithms-principles and examples}},
url = {http://www.imada.sdu.dk/{~}jbj/heuristikker/TSPtext.pdf},
year = {1999}
}
@article{Safavian1991,
abstract = {Decision tree classifiers (DTC's) are used successfully in many diverse areas such as radar signal classification, character recog- nition, remote sensing, medical diagnosis, expert systems, and speech recognition, to name only a few. Perhaps, the most important feature of DTC's is their capability to break down a complex decision-making process into a collection of simpler decisions, thus providing a solution that is often easier to interpret. A survey of current methods for DTC designs and the various existing issues are presented. After considering potential advantages of DTC's over single-state classifiers, the subjects of tree structure design, feature selection at each internal node, and decision and search strategies are discussed. Some remarks concerning the relation between decision trees and neural networks (NN) are also made. I.},
author = {Safavian, S. Rasoul and Landgrebe, David},
doi = {10.1109/21.97458},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Safavian, Landgrebe - 1991 - A Survey of Decision Tree Classifier Methodology.pdf:pdf},
isbn = {0018-9472},
issn = {21682909},
journal = {IEEE Transactions on Systems, Man and Cybernetics},
number = {3},
pages = {660--674},
pmid = {1115},
title = {{A Survey of Decision Tree Classifier Methodology}},
volume = {21},
year = {1991}
}
@article{Kotsiantis2007,
abstract = {Supervised machine learning is the search for algorithms that reason from externally supplied instances to produce general hypotheses, which then make predictions about future instances. In other words, the goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single article cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.},
author = {Kotsiantis, Sotiris B.},
doi = {10.1115/1.1559160},
file = {:home/kelvin/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kotsiantis - 2007 - Supervised machine learning A review of classification techniques.pdf:pdf},
isbn = {1586037803},
issn = {09226389},
journal = {Informatica},
keywords = {algorithms analysis classifiers computational conn,classifiers,data mining techniques,intelligent data analysis,learning algorithms},
pages = {249--268},
title = {{Supervised machine learning: A review of classification techniques}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=vLiTXDHr{\_}sYC{\&}oi=fnd{\&}pg=PA3{\&}dq=survey+machine+learning{\&}ots=CVsyuwYHjo{\&}sig=A6wYWvywU8XTc7Dzp8ZdKJaW7rc{\%}5Cnpapers://5e3e5e59-48a2-47c1-b6b1-a778137d3ec1/Paper/p800{\%}5Cnhttp://www.informatica.si/PDF/31-3/11{\_}Kotsiantis - S},
volume = {31},
year = {2007}
}
